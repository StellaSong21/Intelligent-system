## 智能系统原理与开发 lab 1

[TOC]

### 1. 代码基本架构





### 2. 拟合 sin(x)

#### 2.1 实验参数结果

网络结构设置：[1, 50, 1]，hidden=[[10], [20], [50], [70], [100], [120], [150], [200]]

激活函数：sigmoid 函数

损失函数：均方

weight 初始值：(-1.0 / sqrt(# of input units), 0)

bias 初始值：(-0.2, 0)

weight 和 bias 学习率：0.05

训练集大小：1000

验证集大小：300

测试集大小：300

训练次数：150 次



#### 2.2 不同网络结构、网络参数的实验比较

##### 2.2.1 学习率

> 网络结构设置：[1, 100, 1]，一层 100 个神经元的隐层
>
> 激活函数：sigmoid 函数
>
> 损失函数：均方
>
> weight 初始值：(-1.0 / sqrt(# of input units), 0)
>
> bias 初始值：(-0.1, 0)
>
> weight 和 bias 学习率：[0.1, 0.05, 0.02, 0.01, 0.005]
>
> 训练集大小：1000
>
> 验证集大小：300
>
> 训练次数：40 次

下图表示不同的学习率情况下，平均 loss 随训练次数的变化，并在图例中给出了经过 40 次循环后，训练出的模型在验证集上的损失率。通过对比，学习率取 0.05 较好，损失率曲线更平滑，在验证集上的损失率也低

![learn_rate](.\图片\sin\learn_rate.png)



##### 2.2.2 隐层神经元个数

> 网络结构设置：[1, hidden, 1]，hidden=[[10], [20], [50], [70], [100], [120], [150], [200]]
>
> 激活函数：sigmoid 函数
>
> 损失函数：均方
>
> weight 初始值：(-1.0 / sqrt(# of input units), 0)
>
> bias 初始值：(-0.1, 0)
>
> weight 和 bias 学习率：0.05
>
> 训练集大小：1000
>
> 验证集大小：300
>
> 训练次数：40 次

下图表示不同的神经元个数情况下，平均 loss 随训练次数的变化，并在图例中给出了经过 40 次循环后，训练出的模型在验证集上的损失率。在下图中，不同的隐层神经元个数对损失率变化影响不大，相比而言，隐层神经元有 50 个的时候，损失率变化平滑，在验证集上的损失率也比较小

![hidden](.\图片\sin\hidden.png)



##### 2.2.3 训练次数

> 网络结构设置：[1, 50, 1]，hidden=[[10], [20], [50], [70], [100], [120], [150], [200]]
>
> 激活函数：sigmoid 函数
>
> 损失函数：均方
>
> weight 初始值：(-1.0 / sqrt(# of input units), 0)
>
> bias 初始值：(-0.1, 0)
>
> weight 和 bias 学习率：0.05
>
> 训练集大小：1000
>
> 验证集大小：300
>
> 训练次数：150次

下图表示平均 loss 随训练次数的变化，可以看出 50 次之后，损失率随训练次数下降缓慢，且此后精度已经低于 0.01，因此可以得出结论：在以上参数设置下，训练 70 次即可

![epoch](.\图片\sin\epoch.png)





### 3. 对手写汉字分类

#### 3.1 实验参数结果

网络结构设置：[28 * 28, 150, 12]，一层 250 个神经元的隐层

激活函数：sigmoid 函数

损失函数：交叉熵

weight 初始值：(-1.0 / sqrt(# of input units), 0)

bias 初始值：(-0.1, 0)

weight 的学习率：0.01，每经过 2 个 epoch，就乘以 0.8

bias 的学习率：0.005，每经过 2 个 epoch，就乘以 0.8

训练次数：30 次



#### 3.2 不同网络结构、网络参数的实验比较

##### 3.2.1 weight 学习率

> 网络结构设置：[28 * 28, 250, 12]，一层 250 个神经元的隐层
>
> 激活函数：sigmoid 函数
>
> 损失函数：交叉熵
>
> weight 初始值：(-1.0 / sqrt(# of input units), 0)
>
> bias 初始值：(-0.1, 0)
>
> weight 的学习率：[0.1, 0.05, 0.03, 0.01, 0.005]，每经过 2 个 epoch，就乘以 0.8
>
> bias 的学习率：weight / 2，每经过 2 个 epoch，就乘以 0.8
>
> 训练集和验证集：利用 k-mixture 方法将 TA 给的训练集取 10% 做验证集，其余做训练集
>
> 训练次数：10次

下图表示不同的学习率情况下，平均 loss 随训练次数的变化，并在图例中给出了经过 10 次循环后，训练出的模型在验证集上的精确度。通过对比，weight 的学习率 learn_w 取 0.01 更好，loss 更小，准确率也更高

![learn_w](.\图片\classify\learn_w.png)



##### 3.2.2 bias 学习率

> 网络结构设置：[28 * 28, 250, 12]，一层 250 个神经元的隐层
>
> 激活函数：sigmoid 函数
>
> 损失函数：交叉熵
>
> weight 初始值：(-1.0 / sqrt(# of input units), 0)
>
> bias 初始值：(-0.1, 0)
>
> weight 的学习率：0.01， 每经过 2 个 epoch，就乘以 0.8
>
> bias 的学习率：0.01 * [1.0, 0.67, 0.5, 0.33, 0.25]，每经过 2 个 epoch，就乘以 0.8
>
> 训练集和验证集：利用 k-mixture 方法将 TA 给的训练集取 10% 做验证集，其余做训练集
>
> 训练次数：10 次

下图表示不同的学习率情况下，平均 loss 随训练次数的变化曲线，并在图例中给出了经过 10 次循环后，训练出的模型在验证集上的准确度。从图中可以看出，对 bias 设置不同学习率的差别不是很大，此处更倾向于取 learn_b = 0.05，出于两方面的考虑：一是不会太小，使网络在开始时候调整的速率太慢；二是因为 bias 的值一直是1，0.05 也不会太大，使 bias 对网络的影响过大

![learn_b](.\图片\classify\learn_b.png)



##### 3.2.3 隐层神经元个数

> 网络结构设置：[28 * 28, hidden, 12]，隐层hidden：[[10], [50], [100], [150], [200], [250], [300], [400], [500], [650]]
>
> 激活函数：sigmoid 函数
>
> 损失函数：交叉熵
>
> weight 初始值：(-1.0 / sqrt(# of input units), 0)
>
> bias 初始值：(-0.1, 0)
>
> weight 的学习率：0.01， 每经过 2 个 epoch，就乘以 0.8
>
> bias 的学习率：0.005，每经过 2 个 epoch，就乘以 0.8
>
> 训练集和验证集：利用 k-mixture 方法将 TA 给的训练集取10%做验证集，其余做训练集
>
> 训练次数：10次

下图表示不同的隐层情况下，平均 loss 随训练次数的变化曲线，并在图例中给出了经过 10 次循环后，训练出的模型在验证集上的准确度。通过曲线图可以看出，隐层设为 [100]，[150]，[200]，[250]，[300]的 loss 趋势变化相同，此处取隐层设置为 [250]

![hidden](.\图片\classify\hidden.png)



##### 3.2.4 weight 初始值

> 网络结构设置：[28 * 28, 250, 12]
>
> 激活函数：sigmoid 函数
>
> 损失函数：交叉熵
>
> weight 初始值：[-1.0 / sqrt(# of input units), -1.0,-0.5, -0.3, -0.2, -0.1] *(0, 1)
>
> bias 初始值：(-0.1, 0)
>
> weight 的学习率：0.01， 每经过 2 个 epoch，就乘以 0.8
>
> bias 的学习率：0.005，每经过 2 个 epoch，就乘以 0.8
>
> 训练集和验证集：利用 k-mixture 方法将 TA 给的训练集取10%做验证集，其余做训练集
>
> 训练次数：10次

下图表示weight设置不同初始值的情况下，平均 loss 随训练次数的变化，并在图例中给出了经过50次循环后，训练出的模型在验证集上的精确度。通过对比可以看出，weight 取 -1.0 / sqrt(# of input units) 和 -0.1， -0.2 三种情况下产生的结果很接近，考虑到第一种对网络的适用性更强，取前者

![weight](.\图片\classify\weight.png)



##### 3.2.5 bias 初始值

> 网络结构设置：[28 * 28, 150, 12]
>
> 激活函数：sigmoid 函数
>
> 损失函数：交叉熵
>
> weight 初始值：(-1.0 / sqrt(# of input units), 0)
>
> bias 初始值：[-0.5, -0.2, -0.1, -0.05, -0.02, -0.01] * (-1, 0)
>
> weight 的学习率：0.01， 每经过 2 个 epoch，就乘以 0.8
>
> bias 的学习率：0.005，每经过 2 个 epoch，就乘以 0.8
>
> 训练集和验证集：利用 k-mixture 方法将 TA 给的训练集取10%做验证集，其余做训练集
>
> 训练次数：50 次

下图表示 bias 设置不同初始值的情况下，平均 loss 随训练次数的变化，并在图例中给出了经过50次循环后，训练出的模型在验证集上的精确度。可以看出 bias 取不同的初始值，对结果的影响并不大，此处取 bias=-0.1

![bias](.\图片\classify\bias.png)



##### 3.2.7 学习率变化梯度

> 网络结构设置：[28 * 28, 250, 12]
>
> 激活函数：sigmoid 函数
>
> 损失函数：交叉熵
>
> weight 初始值：(-1.0 / sqrt(# of input units), 0)
>
> bias 初始值：-0.1
>
> dalta_alpha=['`pow(0.6,(num)/2)`', '`pow(0.6,(num)/3)`', '`pow(0.6,(num)/5)`', '`pow(0.6,(num)/10)`',  '`pow(0.8,(num)/2)`', '`pow(0.8,(num)/3)`', '`pow(0.8,(num)/5)`', '`pow(0.8,(num)/15)`',  '`0.8`']
>
> pow(n, (num)/m) 表示每经过 m 个 epoch，就将学习率乘以 n；a 表示每个 epoch 之后将学习率乘以 a
>
> weight 的学习率：0.01
>
> bias 的学习率：0.005
>
> 训练集和验证集：利用 k-mixture 方法将 TA 的训练集取10%做验证集，其余做训练集
>
> 训练次数：50次

下图表示不同的学习率梯度变化情况下，平均 loss 随训练次数的变化，并在图例中给出了经过50次循环后，训练出的模型在验证集上的精确度。可以看出 `pow(0.8, (num)/2)`  的loss变化平滑，准确度更高

![alpha_rate](.\图片\classify\alpha_rate.png)



##### 3.2.6 训练次数

> 网络结构设置：[28 * 28, 250, 12]
>
> 激活函数：sigmoid 函数
>
> 损失函数：交叉熵
>
> weight 初始值：(-1.0 / sqrt(# of input units), 0)
>
> bias 初始值：-0.1
>
> weight 的学习率：0.01，每 2 个 epoch 乘以 0.8
>
> bias 的学习率：0.005，每 2 个 epoch 乘以 0.8
>
> 训练集和验证集：利用 k-mixture 方法将TA给的训练集取10%做验证集，其余做训练集
>
> 训练次数：100 次

下图表示平均 loss 和 准确度 随训练次数的变化：

1. 从loss变化可以看出，训练次数达到30之后，loss几乎不再变化；

2. 从准确度可以看出，在训练次数为 30 左右时，准确度达到峰值

因此，训练次数选择 30

![epoch](.\图片\classify\epoch.png)



### 4. 对反向传播算法的理解

通过这次的实验，我对反向传播算法的理解如下：

1. 反向传播算法可以分为两部分，向前传播和向后传播的过程，向前传播是在当前模型参数下计算对输入产生的结果，向后传播则是根据前面产生的输出结果与期望结果的损失值，调整网络，因此必须在提前对输入打上标签，属于有监督的学习
2. 这次实验很大一部分时间都在调整超参上，BP 神经网络中可以调整的部分有很多，包括网络结构、weight 和 bias 的初始值、学习率、损失函数、激活函数等，如何调整选取或者调整参数，除了要依靠经验之外，还需要了解网络的原理，能够根据输出结果反推参数设计不合理的地方，比如在分类问题中，一开始时候我的测试准确率总是 0.08333 ，无论输入是什么，输出结果总是不变，相当于网络没学到任何东西，去网上搜了一下，才发觉是初始的学习率设得太大，之后再遇到类似的问题，就有经验了