1. Hinton 神经网络

2. 神经网络

3. SVM：

   支持向量机，从某种意义上是逻辑回归算法的强化，通过

4. 线性回归 vs 逻辑回归

   逻辑回归就是在线性回归的结果上加一个 sigmoid 函数，将数值转化为 0 和 1 之间的概率

5. 正态分布，又称高斯分布

   ![img](https://bkimg.cdn.bcebos.com/formula/d8fc1a3696534a47f23d6bcb60c1212c.svg)

   ![img](https://bkimg.cdn.bcebos.com/formula/a49f2d97f625020c180a64346e8cece7.svg)

6. 协方差：协方差为0的两个[随机变量](https://baike.baidu.com/item/随机变量)称为是不相关的。

   ![img](https://bkimg.cdn.bcebos.com/formula/32ab8c25259851a89027c916cc506e27.svg)

7. 分类问题 softmax

8. 线性函数

   - 线性函数可以表达为斜截式：
     $$
     f(x)=mx+b
     $$
     m 是斜率且 m ≠ 0

   - 两个变量之间存在一次函数关系，就称它们之间存在<u>线性关系</u>，正比例关系是线性关系中的特例

9. 预训练，数据归一化

   1.1. 什么是预训练

   - 你需要搭建一个网络模型来完成一个特定的图像分类的任务。首先，你需要随机初始化参数，然后开始训练网络，不断调整直到网络的损失越来越小。在训练的过程中，一开始初始化的参数会不断变化。当你觉得结果很满意的时候，你就可以将训练模型的参数保存下来，以便训练好的模型可以在下次执行类似任务时获得较好的结果。这个过程就是 pre-training。

   深度网络存在问题:

   1. 网络越深，需要的训练样本数越多。若用监督则需大量标注样本，不然小规模样本容易造成过拟合。深层网络特征比较多，会出现的多特征问题主要有多样本问题、规则化问题、特征选择问题。
   2. 多层神经网络参数优化是个高阶非凸优化问题，经常得到收敛较差的局部解；
   3. 梯度扩散问题，BP算法计算出的梯度随着深度向前而显著下降，导致前面网络参数贡献很小，更新速度慢。

   **解决方法：**

    逐层贪婪训练，无监督预训练（unsupervised pre-training）即训练网络的第一个隐藏层，再训练第二个…最后用这些训练好的网络参数值作为整体网络参数的初始值。

   经过预训练最终能得到比较好的局部最优解。

10. batch

11. 正则项：调整

12. HMM

13. CIF

14. 循环神经网络

15. LSTM：长短层记忆网络，long short term memory0

16. **梯度消失和梯度爆炸、梯度扩散(梯度随着深度向前而显著下降，导致前面网络参数贡献很小，更新速度慢)：做lab中体验**

17. 超参数：比如神经网络的层数和每层神经网络的神经元个数以及正则化的一些参数等等，这些参数都是需要人工进行选择的参数，我们称为超参数

